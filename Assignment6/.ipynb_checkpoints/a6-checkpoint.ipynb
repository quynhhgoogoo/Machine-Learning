{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101f5e45",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9ab48670cab023836982c027790f1ed7",
     "grade": false,
     "grade_id": "cell-e229fc13865a2293",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Assignment 6 - Model Complexity & Regularization\n",
    "\n",
    "## Learning goals\n",
    "After successfully completing this assignment, you should understand: \n",
    "* the concept of model complexity on an intuitive level\n",
    "* how one can use regularization to (smoothly) decrease the size of hypothesis space\n",
    "* how to implement regularization by adding a penalty term\n",
    "* how to implement regularization by data augmentation\n",
    "* the relation between adding a penalty-term and data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cefaa7e6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2185ab15d9f55383f41d10f770f3ff27",
     "grade": false,
     "grade_id": "cell-c45bd8cedc5932ac",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False  # enable code auto-completion\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import random\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f2df73",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3f4ce285ee9606d9e4db5b89ee9ff083",
     "grade": false,
     "grade_id": "cell-25aeb7eaabe859eb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Model Complexity\n",
    "In previous assignments, we've seen the basic workflow for applying a supervised machine learning model:\n",
    "\n",
    "- choose a model class and its hyperparameters\n",
    "- fit the model to the training data\n",
    "- use the model to predict labels for new data\n",
    "- assess the model on unseen data (validation set)\n",
    "\n",
    "\n",
    "Now the question of core importance is if our estimator is underperforming, how should we move forward? There are several possible answers:\n",
    "\n",
    "- use a more complex/more flexible model\n",
    "- use a less complex/less flexible model\n",
    "- gather more training samples\n",
    "- gather more data to add features to each sample\n",
    "\n",
    "The answer to this question is often counter-intuitive. In particular, using a more complex model will sometimes give worse results. Consider a contrived dataset where the underlying \"true\" relationship between $y$ and $X$ is a sinusoidal function\n",
    "$y=\\sin(X)+\\varepsilon$, where $\\varepsilon$ is artifically added \"noise\". Note that in reality we would not know the underlying function - that's what we are estimating in the first place!\n",
    "\n",
    "Now let's fit a few different models to this training data. The first model, shown in blue, is linear regression fitted to minimize MSE. The second model, in green, is a polynomial with degree=3. The third one (yellow) is a higher degree polynomial with degree=20. We also include the \"ground truth\" (the function used to generate the data without the noise). Notice that you can see that among these three curves, the polynomial of degree=3 is the closest fit to the underlying sinusoidal relationship.\n",
    "\n",
    "Which model is the most complex/flexible? By flexibility, we mean the ability of the model to fit data. Because the polynomial model with degree=20 can in principle capture both a linear and cubic relationship (by having the higher coefficients zero), we say that the polynomial of degree=20 is the most flexible and the linear model the least flexible. More formally, what's happening here is that the hypothesis space of polynomials up to degree 20 (let's denote it $\\mathcal{H}_{20}$) contains the hypothesis space of polynomials up to degree 3 ($\\mathcal{H}_3\\subset \\mathcal{H}_{20}$), or, equivalently, the effective dimension $d_{\\text{eff}}(\\mathcal{H}_{20})>d_{\\text{eff}}(\\mathcal{H}_3)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ef023a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3c5f51dac4daa107cd2593bbc79b0f2",
     "grade": false,
     "grade_id": "cell-c4000032ff9ffc95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# functions to generate the data in this notebook\n",
    "interval = [0, 7]\n",
    "\n",
    "def generating_function(t):\n",
    "    return np.sin(t) # the intrinsic relation between x and y\n",
    "\n",
    "def make_data(m=30):\n",
    "    '''\n",
    "    generate datapoints with a noisy sinusoidal relationship\n",
    "    '''\n",
    "    np.random.seed(seed=1)\n",
    "    X = (interval[1]-interval[0])*np.random.rand(m, 1)+interval[0]\n",
    "    y = generating_function(X)+0.5*np.random.randn(m, 1) # intrinsic relation + random noise\n",
    "    return X, y\n",
    "\n",
    "# generate the data\n",
    "X, y = make_data(50)\n",
    "t = np.linspace(*interval) # test set\n",
    "ground_truth = generating_function(t) # true label of test set\n",
    "\n",
    "# plot a few different polynomial models\n",
    "fig, axs = plt.subplots(figsize=(10, 7))\n",
    "axs.scatter(X, y, s=7, color='black',label=\"datapoints\")\n",
    "axs.plot(t, ground_truth, label='ground truth', color=\"black\")\n",
    "colors = ['blue', 'green', 'orange']\n",
    "labels = ['linear function', 'polynomial degree=3', 'polynomial degree=20']\n",
    "for i, order in enumerate([1, 3, 20]):\n",
    "    poly = PolynomialFeatures(degree=order, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X)\n",
    "    reg = LinearRegression()\n",
    "    reg = reg.fit(X_poly, y)\n",
    "    axs.plot(t, reg.predict(poly.transform(t.reshape(-1, 1))), # plot the learned functions\n",
    "             color=colors[i], label=labels[i])\n",
    "axs.set_ylim(np.min(ground_truth)-0.5, 2+np.max(ground_truth))\n",
    "axs.set_xlabel('x')\n",
    "axs.set_ylabel('y')\n",
    "axs.set_title('Fitting noisy sinusoidal data with different models')\n",
    "axs.legend(loc='upper left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c190d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88248f4d3461923c8cf70e1fe43061d3",
     "grade": false,
     "grade_id": "cell-b65c651b8a6b408e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "For each of these models we can calculate the **training MSE** and the **validation MSE**. It can be seen in the following plot that the training MSE (given by the red curve) decreases monotonically as the polynomial degree increases. This makes sense, since the polynomial fit can become as flexible as we need it to be in order to minimise **training MSE**, For the **validation MSE** (given by the blue curve) the situation is quite different. The **validation MSE** initially decreases as we increase the polynomial degree but eventually starts to increase again. \n",
    "\n",
    "Why is this? By allowing the model to be extremely complex we are letting it fit to \"patterns\" in the training data. However, as soon as we assess it on new unseen data points (validation set), the model cannot generalise well because these \"patterns\" only fit the training data very well and are not the underlying truth, We are in a situation of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3b17a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9ea988202ce2c0154ece7a20b55fa680",
     "grade": false,
     "grade_id": "cell-5504933ca6bc61db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def train_validate_poly(X_train, y_train, X_val, y_val, model=LinearRegression, degree=5):\n",
    "    # train a polynomial model and validate it\n",
    "\n",
    "    # generate polynomial features\n",
    "    poly_features = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_train_poly = poly_features.fit_transform(X_train)\n",
    "    X_val_poly = poly_features.transform(X_val)\n",
    "\n",
    "    # learn the model and validate it\n",
    "    reg = model()\n",
    "    reg = reg.fit(X_train_poly, y_train)\n",
    "    y_train_pred = reg.predict(X_train_poly)\n",
    "    y_val_pred = reg.predict(X_val_poly)\n",
    "    train_err = mean_squared_error(y_train, y_train_pred)\n",
    "    val_err = mean_squared_error(y_val, y_val_pred)\n",
    "    return reg, train_err, val_err\n",
    "\n",
    "# prepare the data\n",
    "X, y = make_data(50)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=5)\n",
    "train_errs = []\n",
    "val_errs = []\n",
    "\n",
    "# get training and validation error for different degrees\n",
    "orders = range(1, 15)\n",
    "for order in orders:\n",
    "    _, train_err, val_err = train_validate_poly(X_train, y_train, X_val, y_val, degree=order)\n",
    "    train_errs.append(train_err)\n",
    "    val_errs.append(val_err)\n",
    "\n",
    "# create the plot\n",
    "fig, axs = plt.subplots(figsize=(10, 7))\n",
    "axs.plot(orders, train_errs, color='red', label='training error')\n",
    "axs.plot(orders, val_errs, color='blue', label='validation error')\n",
    "axs.set_xlabel('Degree of polynomial (model complexity)')\n",
    "axs.set_ylabel('Mean Squared Error (MSE)')\n",
    "axs.set_title('Effect of model complexity on training and validation error')\n",
    "axs.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7f56fa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2838878209a89e18f8176c897cf50e2",
     "grade": false,
     "grade_id": "cell-1617321b73553f8d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student task A6.1:\n",
    "\n",
    "According to the upper plot, for polynomial degree in $\\{2, 3, 10\\}$, Which one seems to be the best, 2, 3, or 10?\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bccb9b36",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "767cd9ec0d57425807806a14b2e9dd1e",
     "grade": false,
     "grade_id": "cell-d0d987d23edd99d5",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# write down your answer (an int) as:\n",
    "# answer = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "assert int(answer) in {2,3,10}, \"You should pick 2, 3 or 10.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f882bb",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71bdfdb50047ae8863b5e798cee1e12c",
     "grade": true,
     "grade_id": "cell-af3617f306d04f43",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this is the test cell for A6.1, please leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1309dd2d",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8bd0b90b8e6659eb4300202679fc6e25",
     "grade": false,
     "grade_id": "cell-3cd50654a1f92580",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Regularization\n",
    "Based on the previous demo, it is clear that it would be appropriate to prune the hypothesis space. By pruning we mean only considering a subset of the original hypotheses. In this case, it might be viable to only look at models with $\\text{degree} \\leq 3$, as shown by our previous analysis above.\n",
    "\n",
    "However, there are other methods we can use to prune the hypothesis space. We will take a look at two of them: Structural risk minimization (SRM) and data augmentation. Although they are very different at first glance, these two methods are equivalent for linear regression (see sections 7.1 and 7.2 in [MLBook](http://mlbook.cs.aalto.fi)).\n",
    "\n",
    "## Structural risk minimization (SRM)\n",
    "This method tries to reduce model complexity by explicitly penalizing large values of the model parameters. Such a penalty term is known as the regularizer $\\mathcal{R}(h)\\in\\mathbb R_+$, and comes in several forms. The cost function that is minimized when training a regularized model is composed of the average loss and an additional **penalty term**:\n",
    "\n",
    "\\begin{equation}\n",
    " \\hat{\\mathbf{w}}^{(\\lambda)} = \\arg\\min_{\\mathbf{w}\\in\\mathbb R^n}\\left[\\underbrace{\\frac{1}{m}\\sum_{i=1}^{m} \\big(y^{(i)} - h^{(\\mathbf{w})}(\\mathbf{x}^{(i)}) \\big)^{2}}_{\\text{MSE}} + \\underbrace{\\lambda \\mathcal{R}(h)}_{\\text{penalty term}}\\right]\n",
    " \\end{equation}\n",
    "\n",
    "The central idea of regularization is that the penalized cost function is minimized by a less complex predictor. Thus, a model trained using the penalized cost function should, in general, have better generalization capabilities provided that the penalty term is well chosen.\n",
    "\n",
    "The penalty term itself is composed of two factors: a **regularization term** $\\mathcal{R}(h)$ and a scaling factor $\\lambda$ (in some literature denoted by $\\alpha$). The former quantifies a function's complexity, and the latter scales the penalty by a specified factor. Effectively, $\\lambda$ **offers a trade-off between the prediction error (training error) incurred on the training data and the complexity of a predictor**. Large $\\lambda$ favours less complex predictor functions, while small $\\lambda$ puts more emphasis on obtaining a small average loss.\n",
    "\n",
    "Here we use Ridge regression to illustrate this. Ridge regression is Linear regressions with L2 regularization:\n",
    "$$\n",
    "\\hat{\\mathbf{w}}^{(\\lambda)} = \\arg\\min_{\\mathbf{w}\\in\\mathbb R^n}\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\left(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}\\right)+\\lambda \\|\\mathbf{w}\\|^2_2\\right]=\n",
    "\\arg\\min_{\\mathbf{w}\\in\\mathbb R^n}\\left[\\frac{1}{m}\\sum_{i=1}^{m}\\big(y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)}\\big)+\\lambda \\sum_{j=1}^{n} \\mathbf{w}_j^{2}\\right]\n",
    "$$\n",
    "\n",
    "From the plot below, it's shown that as $\\lambda$ increases (model complexity decreases), training error keeps increasing while validation error decreases, but eventually starts to increase again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83af173e",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8a2120593c1d971c16d7cbfac5f60d2b",
     "grade": false,
     "grade_id": "cell-ff067bf4cb31e0fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# create the data\n",
    "X, y = make_data(50)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=2)\n",
    "degree = 15\n",
    "\n",
    "# iteratively try different lambdas\n",
    "regs = []\n",
    "train_errs = []\n",
    "val_errs = []\n",
    "l_range = (-3, 20)\n",
    "lambdas = np.logspace(*l_range, num=200)  # candidate lambdas\n",
    "for l in lambdas:\n",
    "    reg, train_err, val_err = train_validate_poly(\n",
    "        X_train, y_train, X_val, y_val, lambda: Ridge(alpha=l, solver='svd'), degree=degree)\n",
    "    regs.append(reg)\n",
    "    train_errs.append(train_err)\n",
    "    val_errs.append(val_err)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "\n",
    "# plot the data\n",
    "plt.setp(axs, xscale='log', xticks=[10**x for x in range(*l_range)], xlabel='lambda', ylabel='Mean Squared Error (MSE)')\n",
    "axs[0].plot(lambdas, train_errs, color='red', label='training')\n",
    "axs[0].plot(lambdas, val_errs, color='blue', label='validation')\n",
    "axs[0].set_title('Effect of lambda on training and validation error')\n",
    "axs[0].legend()\n",
    "\n",
    "axs[1].set_yscale('log')\n",
    "axs[1].plot(lambdas, [np.linalg.norm(reg.coef_) for reg in regs])\n",
    "axs[1].set_ylabel('Weight norm $||w||_2$')\n",
    "axs[1].set_title('Effect of lambda on L2 norm of model weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155f9ae3",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7a270b1687fa31c4f5773f269d5ca628",
     "grade": false,
     "grade_id": "cell-024c5410f665de2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "# Student Task A6.2:\n",
    "\n",
    "According to the upper plot, which lambda out of the set $\\{0.1,1,10,100,1000,10000\\}$ would you choose?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d43d296",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94ef19dc2ed162fcdb4ce2721b39de4d",
     "grade": false,
     "grade_id": "cell-9dd12f763f182ae0",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# write down your answer as:\n",
    "# answer = ...\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "assert answer in {0.1,1,10,100,1000,10000}, \"Invalid answer, pick one from the set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087bd157",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "78818bec765b6173fc3ed6c3ca408f49",
     "grade": true,
     "grade_id": "cell-b080ddea5f16bfd7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "#this cell is for test of A6.2, please leave it as it is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bdcc6b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b934d5d523a976d27331904dbf640e14",
     "grade": false,
     "grade_id": "cell-23ade112e7a59672",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## Data augmentation\n",
    "One way to combat overfitting is to get more training data. Unfortunately, this is not always feasible, be it due to budget constraints (need to pay people to label the data) or time constraints. The idea behind data augmentation is to use datapoints that are already available to generate new ones, thus enlarging the training dataset.\n",
    "\n",
    "When dealing with image classification, new images can be generated e.g. by rotating, cropping or changing pixel values slightly, since these don't change the class (flipping a dog upside down doesn't make it a cat). Our assignment problem is simpler and we will augment the data by adding perturbations generated by a normal distribution with zero mean, $\\varepsilon\\sim\\mathcal{N}(0,\\sigma^2)$, to our training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a368543",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f4022ee16c70a5d00a60885f47f6b36",
     "grade": false,
     "grade_id": "cell-60e13b754130697a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student task A6.3:\n",
    "Write a function ``augment_data`` that takes numpy arrays $X=[x_1,x_2,\\dots,x_m]^T$ and $y=[y_1,y_2,\\dots,y_m]^T$, both of which of shape ``(m,1)``, a positive integer ``n_perturbations`` and a float ``std`` and returns numpy arrays $X_{\\text{augmented}}$ and $y_{\\text{augmented}}$ of shape ``(n_perturbations*m, 1)`` that are created as follows. \n",
    "\n",
    "First generate a randome vector of perturbations $\\varepsilon=(\\varepsilon_1, \\dots, \\varepsilon_n)$, where ``n=n_perturbations``, each of which is sampled from a normal distribution with zero mean and standard deviation ``std``. Then, create a new array $X_{\\text{augmented}}=[X^T+\\epsilon_1, X^T+\\epsilon_2,\\dots,X^T+\\epsilon_n]^T$. Note that we used a slight abuse of notation $X+\\epsilon_i:=[x_1+\\varepsilon_i, x_2+\\varepsilon_i,\\dots,x_m+\\varepsilon_i]^T$. This is something numpy calls [broadcasting](https://numpy.org/doc/stable/user/basics.broadcasting.html) and you can use it to make your code simpler. Array ``y_augmented`` is created simply by having ``y`` repeated ``n``-times, so that the perturbed datapoints still correspond to the same labels.  \n",
    "\n",
    "Tips:\n",
    "* The [numpy.random.normal](https://numpy.org/doc/stable/reference/random/generated/numpy.random.normal.html) and [numpy.concatenate](https://numpy.org/doc/stable/reference/generated/numpy.concatenate.html?highlight=concatenate#numpy.concatenate) functions might be useful for this task.\n",
    "* Use the view_augmentation function to check if your function does what it should. Check the L7 slides, p. 37 and compare the plots. **Do they look similar?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5e0f8bba593ee10aca20ccc5cb54ba96",
     "grade": false,
     "grade_id": "cell-9598a0ef85da8f84",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def augment_data(X, y, n_perturbations, std):\n",
    "    \"\"\"\n",
    "    Data augmentation function.\n",
    "    inputs: X, the original feature matrix, X.shape=(m,1)\n",
    "            y, the original featrue matrix, y.shape=(m,1)\n",
    "            n_perturbations, number of perturbations, int\n",
    "            std, standard deviation of perturbations, float\n",
    "            \n",
    "    outputs: X_augmented, the augmented feature matrix, X_augmented.shape=(m*n,1)\n",
    "             y_augmented, the augmented labels, y_augmented.shape=(m*n,1)\n",
    "    \"\"\"\n",
    "    ## generate the perturbations\n",
    "    # perturbations = np.random.normal(...) # a random vector with n elements, i.e. size=n\n",
    "    \n",
    "    # aug_Xs = [X+pert for pert in perturbations] # a lsit of arrays, each array is an augmented X with a particular perturbation\n",
    "    # aug_ys = [y for pert in perturbations] # a list of augmented y, Note: no value changes for y\n",
    "\n",
    "    ## generate the augmented training set, the number of datapoints is m*n\n",
    "    # X_augmented = np.concatenate(...)  # concatenate augmented Xs vertically, i.e. along axis 0\n",
    "    # y_augmented = np.concatenate(...)  # concatenate augmented ys vertically, i.e. along axis 0\n",
    "    \n",
    "    # return X_augmented, y_augmented\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "\n",
    "\n",
    "def view_augmentation(n_perturbations=10, std=1e-1):\n",
    "    # generate data\n",
    "    X, y = make_data(20)\n",
    "\n",
    "    # illustrate augmentation\n",
    "    plt.figure()\n",
    "    plt.title(\"Augmentation illustration\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "\n",
    "    plt.scatter(*augment_data(X, y, n_perturbations, std), label=\"augmented datapoints\")\n",
    "    plt.scatter(X, y, label=\"original datapoints\")\n",
    "    plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4e7a4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "951159f163c3b4bd2e158c3cc1e8d839",
     "grade": false,
     "grade_id": "cell-9598a0ef75da8f84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert augment_data(np.zeros((5, 1)), np.zeros((5, 1)), 10, 0.1)[0].shape == (50, 1), \"Incorrect shape of X_augmented\"\n",
    "assert augment_data(np.zeros((5, 1)), np.zeros((5, 1)), 10, 0.1)[1].shape == (50, 1), \"Incorrect shape of y_augmented\"\n",
    "view_augmentation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7a70b6",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d410250da85aa3d50afaec4fabc0602",
     "grade": true,
     "grade_id": "cell-3724a91681c5a109",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# this cell is for test of A6.3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e65f4",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ec28c45de37ef5a5861103fa7bf32dd0",
     "grade": false,
     "grade_id": "cell-93e8c819a3a623e1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "In the cell below, we use the ``augment_data`` method you implemented to show the dependence between augmentation standard deviation and validation error for polynomials models of different degrees. Because we're using a low number of datapoints, the trained models can change significantly even with small changes in the data, which leads to \"noisy\" curves. We therefore use a median filter to make them smoother and make the plot clearer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4730eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Note: this cell is editable for you to try out different values for degree and sigma\"\"\"\n",
    "# generate data\n",
    "from scipy.signal import medfilt\n",
    "X, y = make_data(50)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "smoothing = 9\n",
    "\n",
    "# generate plot data\n",
    "plot_data = []\n",
    "sigmas = np.logspace(-2, 2, 250)\n",
    "degrees = [5, 10, 15, 20]\n",
    "for deg in degrees:\n",
    "    train_errs = []\n",
    "    val_errs = []\n",
    "    regs = []\n",
    "    for sigma in sigmas:\n",
    "        reg, train_err, val_err = train_validate_poly(\n",
    "            *augment_data(X_train, y_train, 10, sigma),\n",
    "            X_val, y_val, degree=deg)\n",
    "        train_errs.append(train_err)\n",
    "        val_errs.append(val_err)\n",
    "        regs.append(reg)\n",
    "    plot_data.append({\"deg\": deg, \"x\": sigma, \"train_errs\": train_errs, \"val_errs\": val_errs,\n",
    "                     \"weight_norms\": [np.linalg.norm(reg.coef_, 1) for reg in regs]})\n",
    "\n",
    "# plots wrt. number of augmented datapoints\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 12))\n",
    "for p in plot_data:\n",
    "    axs[0].plot(sigmas, medfilt(p[\"val_errs\"], smoothing), label=f'degree={p[\"deg\"]}')\n",
    "    axs[1].plot(sigmas, medfilt(p[\"weight_norms\"], smoothing), label=f'degree={p[\"deg\"]}')\n",
    "axs[1].set_yscale('log')\n",
    "axs[0].set_xscale('log')\n",
    "axs[1].set_xscale('log')\n",
    "axs[0].set_xlabel('$\\sigma$')\n",
    "axs[0].set_ylabel('MSE on the validation set')\n",
    "axs[0].set_title('Effect of $\\sigma$ on Validation error for different degrees of polynomials')\n",
    "axs[0].legend()\n",
    "axs[1].set_xlabel('$\\sigma$')\n",
    "axs[1].set_ylabel('Weight norm $||w||_2$')\n",
    "axs[1].set_title('Effect of $\\sigma$ on L2 norm of weights for different degrees of polynomials')\n",
    "axs[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4752d36c",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f712cf124462d8e614702f9b5a8d0ec9",
     "grade": false,
     "grade_id": "cell-67cb3ef704f27f95",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<div class=\" alert alert-warning\">\n",
    "\n",
    "# Student task A6.4:\n",
    "Carefully observe the upper plots and play with the parameters in the previous cell for a bit to answer the following questions (especially Q1 and Q2): \n",
    "* Q1: Is the polynomial model of degree 5 guaranteed to always have lower validation error than the polynomial of degree 20, no matter the augmentation standard deviation? \n",
    "    * 0: No\n",
    "    * 1: Yes\n",
    "* Q2: When is data augmentation most useful? \n",
    "    * 0: When there's not enough training data.\n",
    "    * 1: When there's too much training data.\n",
    "    * 2: This does not depend on training data quantity.\n",
    "* Q3: Why is the validation error very similar for small standard deviation of the added noise as it is for the original data?\n",
    "    * 0: The model that fits the original data fits such augmented data almost equally well.\n",
    "    * 1: The model gets confused by too many similar datapoints.\n",
    "    * 2: The augmented data is not representative of the original data. \n",
    "* Q4: Why does the validation error start to rise again when we keep increasing the standard deviation of the added noise?\n",
    "    * 0: There's too many datapoints.\n",
    "    * 1: The augmented data starts to be too similar to the original data again.\n",
    "    * 2: The augmented data is no longer representative of the original data. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1994c8e1",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7b2ff7cabcb1991f4178f1b1d250656c",
     "grade": false,
     "grade_id": "cell-eb3ad16f82008cb6",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A1=... # 0: No, 1: Yes\n",
    "# A2=... # 0: When there's not enough training data, 1: When there's too much training data, 2: This does not depend on training data.\n",
    "# A3=... # 0: The model that fits the original data fits such augmented data almost equally well, 1: The model gets confused by too many similar datapoints, 2: The augmented data is not representative of the original data. \n",
    "# A4=... # 0: There's too many datapoints, 1: The augmented data starts to be too similar to the original data again, 2: The augmented data is no longer representative of the ground truth.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "assert A1 in {0, 1}\n",
    "assert A2 in {0, 1, 2}\n",
    "assert A3 in {0, 1, 2}\n",
    "assert A4 in {0, 1, 2}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90221f7a81d985dcacb5e2dd689af732",
     "grade": true,
     "grade_id": "cell-1f4fff2fb81fecd3",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for A6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8520f5189033a8f8cc49a88409bc6d4",
     "grade": true,
     "grade_id": "cell-5bf5a962ecb7be50",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for A6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "134368c5ab45d32c3cf71db69b7e530b",
     "grade": true,
     "grade_id": "cell-a966173de5ee5027",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for A6.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d21c163bf65597ec73010d1fd89b4a5f",
     "grade": true,
     "grade_id": "cell-12b4a351919d66a4",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# test cell for A6.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab43de2",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "151fcd057c1f518ea042bff1921d69c3",
     "grade": false,
     "grade_id": "cell-c131c4da7b9405a1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Demo\n",
    "As you can see in the plot below, adding regularization significantly helps the problem of increasing validation error for higher polynomial degrees. Note, however, that a model with polynomial degree 5 still manages to get a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a81aa",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d853aa7da176e6dae4179d51450c239c",
     "grade": false,
     "grade_id": "cell-54fcecacb5735187",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X, y = make_data(40)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.33, random_state=2)\n",
    "val_errs = []\n",
    "val_errs_ridge = []\n",
    "val_errs_augmentation = []\n",
    "regs = []\n",
    "regs_ridge = []\n",
    "regs_augmentation = []\n",
    "\n",
    "# generate the data\n",
    "max_deg = 15\n",
    "orders = range(1, max_deg)\n",
    "for order in orders:\n",
    "    reg, _, val_err = train_validate_poly(X_train, y_train, X_val, y_val, degree=order)\n",
    "    reg_ridge, _, val_err_ridge = train_validate_poly(X_train, y_train, X_val, y_val,\n",
    "                                                      lambda: Ridge(alpha=1e3, solver='svd'), degree=order)\n",
    "    reg_augmentation, _, val_err_augmentation = train_validate_poly(\n",
    "        *augment_data(X_train, y_train, order, 0.8), X_val, y_val, degree=order)\n",
    "    val_errs.append(val_err)\n",
    "    val_errs_ridge.append(val_err_ridge)\n",
    "    val_errs_augmentation.append(val_err_augmentation)\n",
    "    regs.append(reg)\n",
    "    regs_ridge.append(reg_ridge)\n",
    "    regs_augmentation.append(reg_augmentation)\n",
    "reg5, _, val_err5 = train_validate_poly(X_train, y_train, X_val, y_val, degree=5)\n",
    "\n",
    "# plot\n",
    "fig, axs = plt.subplots(2, 1, figsize=(15, 12))\n",
    "axs[0].plot(orders, val_errs, color='red', label='no regularization')\n",
    "axs[0].plot(orders, val_errs_ridge, color='blue', label='ridge regularization')\n",
    "axs[0].plot(orders, val_errs_augmentation, color='green', label='augmentation regularization')\n",
    "axs[0].axhline(y=val_err5,color=\"orange\",linestyle='--',label=\"no regularization, deg=5\")\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xticks(np.arange(1, max_deg))\n",
    "axs[0].set_xlabel('Polynomial features degree')\n",
    "axs[0].set_ylabel('Validation error')\n",
    "axs[0].set_title('Effect of regularization on validation error')\n",
    "axs[0].legend()\n",
    "\n",
    "\n",
    "t = np.linspace(*interval, 1000)\n",
    "ground_truth = generating_function(t)\n",
    "t_poly = PolynomialFeatures(degree=max_deg-1, include_bias=False).fit_transform(t.reshape(-1, 1))\n",
    "t_poly5 = PolynomialFeatures(degree=5, include_bias=False).fit_transform(t.reshape(-1, 1))\n",
    "\n",
    "# plot a few different polynomial models\n",
    "axs[1].scatter(X, y, s=7, color='black')\n",
    "axs[1].plot(t, ground_truth, label='ground truth', color=\"black\")\n",
    "\n",
    "axs[1].plot(t, regs[-1].predict(t_poly), color='red', label='no regularization')\n",
    "axs[1].plot(t, regs_ridge[-1].predict(t_poly), color='blue', label='ridge regularization')\n",
    "axs[1].plot(t, regs_augmentation[-1].predict(t_poly), color='green', label='augmentation regularization')\n",
    "axs[1].plot(t, reg5.predict(t_poly5), color='orange', label='no regularization, deg=5')\n",
    "\n",
    "axs[1].set_ylim(np.min(ground_truth)-0.5, 2+np.max(ground_truth))\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "axs[1].set_title(f'Effect of regularization on polynomial features of degree {max_deg-1}')\n",
    "axs[1].legend(loc='upper left')\n"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
